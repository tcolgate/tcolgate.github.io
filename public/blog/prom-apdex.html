<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">.lst-kix_dohk24d5rzuj-8>li:before{content:"\0025a0  "}.lst-kix_1n0xs9po7auh-8>li:before{content:"\0025a0  "}ul.lst-kix_dohk24d5rzuj-0{list-style-type:none}ul.lst-kix_dohk24d5rzuj-2{list-style-type:none}ul.lst-kix_dohk24d5rzuj-1{list-style-type:none}.lst-kix_dohk24d5rzuj-7>li:before{content:"\0025cb  "}.lst-kix_1n0xs9po7auh-7>li:before{content:"\0025cb  "}.lst-kix_1n0xs9po7auh-4>li:before{content:"\0025cb  "}.lst-kix_1n0xs9po7auh-6>li:before{content:"\0025cf  "}.lst-kix_1n0xs9po7auh-5>li:before{content:"\0025a0  "}.lst-kix_dohk24d5rzuj-0>li:before{content:"\0025cf  "}.lst-kix_dohk24d5rzuj-2>li:before{content:"\0025a0  "}.lst-kix_1n0xs9po7auh-0>li:before{content:"\0025cf  "}.lst-kix_1n0xs9po7auh-2>li:before{content:"\0025a0  "}.lst-kix_dohk24d5rzuj-3>li:before{content:"\0025cf  "}.lst-kix_1n0xs9po7auh-3>li:before{content:"\0025cf  "}ul.lst-kix_dohk24d5rzuj-4{list-style-type:none}ul.lst-kix_dohk24d5rzuj-3{list-style-type:none}ul.lst-kix_dohk24d5rzuj-6{list-style-type:none}.lst-kix_dohk24d5rzuj-4>li:before{content:"\0025cb  "}ul.lst-kix_dohk24d5rzuj-5{list-style-type:none}.lst-kix_dohk24d5rzuj-6>li:before{content:"\0025cf  "}ul.lst-kix_dohk24d5rzuj-8{list-style-type:none}ul.lst-kix_dohk24d5rzuj-7{list-style-type:none}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_dohk24d5rzuj-5>li:before{content:"\0025a0  "}.lst-kix_1n0xs9po7auh-1>li:before{content:"\0025cb  "}ul.lst-kix_1n0xs9po7auh-7{list-style-type:none}ul.lst-kix_1n0xs9po7auh-6{list-style-type:none}ul.lst-kix_1n0xs9po7auh-8{list-style-type:none}ul.lst-kix_1n0xs9po7auh-3{list-style-type:none}ul.lst-kix_1n0xs9po7auh-2{list-style-type:none}ul.lst-kix_1n0xs9po7auh-5{list-style-type:none}ul.lst-kix_1n0xs9po7auh-4{list-style-type:none}ul.lst-kix_1n0xs9po7auh-1{list-style-type:none}ul.lst-kix_1n0xs9po7auh-0{list-style-type:none}.lst-kix_dohk24d5rzuj-1>li:before{content:"\0025cb  "}ol{margin:0;padding:0}table td,table th{padding:0}.c6{background-color:#ffffff;margin-left:59pt;padding-top:32pt;padding-left:0pt;padding-bottom:-5pt;line-height:1.9090909090909092;orphans:2;widows:2;text-align:left}.c13{background-color:#ffffff;margin-left:59pt;padding-top:17pt;padding-left:0pt;padding-bottom:-5pt;line-height:1.9090909090909092;orphans:2;widows:2;text-align:left}.c1{background-color:#f2f2f2;color:#292929;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Courier New";font-style:normal}.c18{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c15{background-color:#ffffff;padding-top:40pt;padding-bottom:-5pt;line-height:1.0588235294117647;orphans:2;widows:2;text-align:left}.c9{background-color:#ffffff;padding-top:30pt;padding-bottom:-7pt;line-height:2.1818181818181817;orphans:2;widows:2;text-align:left}.c10{background-color:#ffffff;padding-top:13pt;padding-bottom:-7pt;line-height:2.1818181818181817;orphans:2;widows:2;text-align:left}.c7{background-color:#ffffff;padding-top:14pt;padding-bottom:-6pt;line-height:1.3043478260869565;orphans:2;widows:2;text-align:left}.c2{padding-top:41pt;padding-bottom:-1pt;line-height:1.18;orphans:2;widows:2;text-align:left}.c22{padding-top:42pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c4{font-size:15pt;font-family:"Georgia";color:#292929;font-weight:700}.c0{font-size:15pt;font-family:"Georgia";color:#292929;font-weight:400}.c8{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c14{color:#000000;font-weight:400;font-size:11pt;font-family:"Arial"}.c17{color:#292929;font-weight:700;font-size:15pt;font-family:"Arial"}.c21{font-weight:700;font-size:24pt;font-family:"Arial"}.c3{text-decoration:none;vertical-align:baseline;font-style:normal}.c11{font-weight:400;font-size:15pt;font-family:"Georgia"}.c24{font-weight:400;font-size:11pt;font-family:"Arial"}.c16{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c12{color:inherit;text-decoration:inherit}.c5{padding:0;margin:0}.c23{font-style:italic}.c20{height:11pt}.c19{color:#292929}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c16 doc-content"><p class="c7"><span class="c19">(originally post here </span><span class="c8"><a class="c12" href="https://www.google.com/url?q=https://medium.com/@tristan_96324/prometheus-apdex-alerting-d17a065e39d0&amp;sa=D&amp;source=editors&amp;ust=1679579792426771&amp;usg=AOvVaw1c7r3a_L0zkVQ3eMAEVjR-">https://medium.com/@tristan_96324/prometheus-apdex-alerting-d17a065e39d0</a></span><span class="c3 c19 c24">)</span></p><p class="c18"><span class="c3 c14"></span></p><h1 class="c7" id="h.kxrpi27vfhnf"><span class="c3 c19 c21">Prometheus: Apdex alerting</span></h1><p class="c9"><span class="c0 c3">I&rsquo;m going to take a break from our series on Production Kubernetes to write up a couple of smaller articles on Prometheus. This article will give an overview of Apdex, and go on to describe a practical approach to implementing Apdex alerts in Prometheus.</span></p><h2 class="c15" id="h.ofysjmnxc943"><span class="c17 c3">Why Apdex?</span></h2><p class="c10"><span class="c0">Apdex provides a single number that attempts to quantify a user&rsquo;s experience of requests to a web service. We first decide what we consider to be an acceptable response time from our service, in Apdex terminology this is called the </span><span class="c4">T </span><span class="c0 c3">value. We then classifies requests as follows:</span></p><ul class="c5 lst-kix_1n0xs9po7auh-0 start"><li class="c6 li-bullet-0"><span class="c0 c3">All error responses are intolerable.</span></li><li class="c13 li-bullet-0"><span class="c0">All responses qucker than </span><span class="c4">T </span><span class="c0 c3">are satisfactory.</span></li><li class="c13 li-bullet-0"><span class="c0">All responses slower than </span><span class="c4">T</span><span class="c0">, but</span><span class="c4">&nbsp;</span><span class="c0">quicker than </span><span class="c4">4T </span><span class="c0 c3">are tolerable.</span></li><li class="c13 li-bullet-0"><span class="c0">All responses slower than </span><span class="c4">4T</span><span class="c0 c3">&nbsp;are intolerable.</span></li></ul><p class="c9"><span class="c0 c3">The Apdex score for a service, over a given time, is a ratio of intolerable vs satisfactory and tolerable responses.</span></p><p class="c2"><span class="c1">apdex = (satisfactory + (tolerable / 2)) / total</span></p><p class="c9"><span class="c0 c3">High error rates and slow response times will result in a lower Apdex score. By encapsulating errors and high latency in a single metric Apdex attempts to quantify our users experience of our service as a single number. Since a low Apdex score implies either high rates of errors or undesirable response latencies it is a reliable metric to alarm on. While a low Apdex score reliably indicates issues, depending on where it is measured from, a high score may not guarantee all is well.</span></p><h2 class="c15" id="h.g011yv5vescu"><span class="c17 c3">Application metrics</span></h2><p class="c10"><span class="c0">Since Apdex represents user experience we would ideally measure it externally from the application. Load balancer metrics are a good choice, (</span><span class="c8 c11"><a class="c12" href="https://www.google.com/url?q=https://traefik.io/&amp;sa=D&amp;source=editors&amp;ust=1679579792429146&amp;usg=AOvVaw3YORhNEtpDx6g020LoU4v8">Traefik</a></span><span class="c0 c3">&nbsp;provides all the required metrics). Apdex can also be measured directly from the application as long as we keep in mind that we are not guaranteed to be seeing all errors. Refused connection are an example of an error that is probably not measurable from inside the application. On the other hand, application side measurement allows us more control over the metrics we produce.</span></p><p class="c9"><span class="c0">You can find a simple example Go application with suitably instrumented handlers, along with Prometheus configuration files, </span><span class="c8 c11"><a class="c12" href="https://www.google.com/url?q=https://github.com/tcolgate/prometheus-rules/tree/master/apdex&amp;sa=D&amp;source=editors&amp;ust=1679579792429698&amp;usg=AOvVaw2LvKQtjCiwMhJ1M8PwgT3C">here</a></span><span class="c0 c3">. The instrumentation provide the following metrics:</span></p><ul class="c5 lst-kix_dohk24d5rzuj-0 start"><li class="c6 li-bullet-0"><span class="c4">http_apdex_target_seconds</span><span class="c0 c23">&nbsp;</span><span class="c0 c3">is a gauge metric providing our target request duration.</span></li><li class="c13 li-bullet-0"><span class="c4">http_request_duration_seconds </span><span class="c0 c3">is a histogram of response times with handler name and response code labels.</span></li></ul><p class="c9"><span class="c0 c3">The Prometheus documentation rightly cautions against adding too many labels to metrics. Including the response code and handler as a labels is questionable. Request code are arbitrary (though usually between 200 and 599) and including them can result in an explosion of the number of time series your application is reporting. In addition to this, it has been suggested that differentiating errors with labels can lead poor dashboarding and alerting by allowing people to only focus on successful calls. For the purpose of Apdex calculation we need to differentiate errors from successes. In our example we use the response code, other approaches are possible. In practice your author has not seen problems including code and handler as labels on production services.</span></p><p class="c9"><span class="c0">Since we will probably want to our histogram for more than just Apdex calculations, we include some additional buckets. The only requirement is that we must have buckets at exactly </span><span class="c4">T </span><span class="c0">and</span><span class="c4">&nbsp;4T</span><span class="c0 c3">, other buckets are included here to demonstrate that they do not impact the overall calculation.</span></p><h2 class="c15" id="h.3bdzwralbqx0"><span class="c17 c3">Calculating Apdex</span></h2><p class="c10"><span class="c0 c3">The approach given below uses some relatively advanced Prometheus query techniques which I hope you will find interesting and useful in future. We will walk through each rule, giving the motivation and a brief discussion of how it works. First off let us look at our raw material, what metrics is our application producing (I have filtered out series that we do not need for our calculations).</span></p><p class="c2"><span class="c1">http_request_apdex_target_seconds 0.1</span></p><p class="c2"><span class="c1">http_request_duration_seconds_bucket{</span></p><p class="c2"><span class="c1">&nbsp;code=&quot;200&quot;,handler=&quot;/healthz&quot;,le=&quot;0.1&quot;} 10</span></p><p class="c2"><span class="c1">http_request_duration_seconds_bucket{</span></p><p class="c2"><span class="c1">&nbsp;code=&quot;200&quot;,handler=&quot;/healthz&quot;,le=&quot;0.4&quot;} 10</span></p><p class="c2"><span class="c1">http_request_duration_seconds_bucket{</span></p><p class="c2"><span class="c1">&nbsp;code=&quot;200&quot;,handler=&quot;/healthz&quot;,le=&quot;+Inf&quot;} 10</span></p><p class="c2"><span class="c1">http_request_duration_seconds_bucket{</span></p><p class="c2"><span class="c1">&nbsp;code=&quot;200&quot;,handler=&quot;/q&quot;,le=&quot;0.1&quot;} 775</span></p><p class="c2"><span class="c1">http_request_duration_seconds_bucket{</span></p><p class="c2"><span class="c1">&nbsp;code=&quot;200&quot;,handler=&quot;/q&quot;,le=&quot;0.4&quot;} 1605</span></p><p class="c2"><span class="c1">http_request_duration_seconds_bucket{</span></p><p class="c2"><span class="c1">&nbsp;code=&quot;500&quot;,handler=&quot;/q&quot;,le=&quot;0.1&quot;} 390</span></p><p class="c2"><span class="c1">http_request_duration_seconds_bucket{</span></p><p class="c2"><span class="c1">&nbsp;code=&quot;500&quot;,handler=&quot;/q&quot;,le=&quot;0.4&quot;} 390</span></p><p class="c2"><span class="c1">http_request_duration_seconds_count{</span></p><p class="c2"><span class="c1">&nbsp;code=&quot;500&quot;,handler=&quot;/q&quot;} 390</span></p><p class="c9"><span class="c0 c3">Since each instance of the application may expose a different value for any metric, we must establish what the current preferred T value is. We shall use the current maximum for a given job.</span></p><p class="c2"><span class="c1">- record: job:http_request_apdex_target_seconds:max</span></p><p class="c2"><span class="c1">&nbsp;expr: max(http_request_apdex_target_seconds) BY (job)</span></p><p class="c22"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 396.00px;"><img alt="" src="images/image1.png" style="width: 624.00px; height: 396.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c0">Our new metric is convenient for adding to graphs to indicate the expected latency, but we must adjust it to be useful for alerting. Prometheus histograms use a label (</span><span class="c4">le</span><span class="c0">) to label each bucket. We are going to want to be able to select out buckets based on our </span><span class="c4">T</span><span class="c0 c3">&nbsp;value. We could have simply produced some metrics with the required labels, but as alternative, we can also use the following rules to produce the required series on the Prometheus server side</span></p><p class="c2"><span class="c1">- record: job_le:http_request_apdex_target_seconds:max</span></p><p class="c2"><span class="c1">&nbsp;expr: |</span></p><p class="c2"><span class="c1">&nbsp; &nbsp;clamp_max(</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp;count_values(</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp;&quot;le&quot;,</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp;job:http_request_apdex_target_seconds:max</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp;) BY (job),</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp;1)</span></p><p class="c2 c20"><span class="c1"></span></p><p class="c2"><span class="c1">- record: job_le:http_request_apdex_target_seconds:4_times_max</span></p><p class="c2"><span class="c1">&nbsp;expr: |</span></p><p class="c2"><span class="c1">&nbsp; &nbsp;clamp_max(</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp;count_values(</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp;&quot;le&quot;,</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp;job:http_request_apdex_target_seconds:max * 4</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp;) BY (job),</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp;1)</span></p><p class="c9"><span class="c0">These rules transform our selected </span><span class="c4">job:http_apdex_target_seconds:max </span><span class="c0">in to two new metrics. These represent </span><span class="c4">T </span><span class="c0">and </span><span class="c4">4T</span><span class="c0">. Rather than simply having a metric with the specific values, </span><span class="c4">count_values </span><span class="c0">transforms the value of a time series into a label, </span><span class="c4">le </span><span class="c0">in our case</span><span class="c4">,</span><span class="c0 c3">with the value of the label being a string representation of the values. Since we know we only have 1 occurrence of this metric for a given instance of our application, we know the count we always be 1. This results in the following metrics</span></p><p class="c2"><span class="c1">job_le:http_request_apdex_target_seconds:max{le=&quot;0.1&quot;} = 1</span></p><p class="c2"><span class="c1">job_le:http_request_apdex_target_seconds:4_times_max{le=&quot;0.4&quot;} = 1</span></p><p class="c9"><span class="c0">The </span><span class="c4">le </span><span class="c0 c3">label can now be used for label matching in calculations involving our histogram. We can finally calculate our Apdex score. We will adjust the formula slightly</span></p><p class="c2"><span class="c1">apdex = (</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; (</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; satisfactoryRate +</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (satisfactor + tolerableRate)</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; ) / 2</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp;) / totalRate</span></p><p class="c9"><span class="c0">We use rates rather than total requests. We could have used </span><span class="c4">increase() </span><span class="c0">in place of </span><span class="c4">rate()</span><span class="c0">, however Prometheus estimates increases by multiply rates by the requested duration, the end result is equivalent. Also, since our histogram buckets count all samples with duration less than or equal to the specific bucket, the </span><span class="c4">4T</span><span class="c0">&nbsp;bucket will also include sample from the </span><span class="c4">T</span><span class="c0 c3">&nbsp;bucket. We could subtract , but we can also just move the division.</span></p><p class="c9"><span class="c0 c3">For low traffic services it is often desirable to ignore our own health checks. Health checks are useful for establishing if a service is capable of serving traffic, but rarely catch unexpected errors. They often only trivially test back end services, so are unlikely to timeout in the way that bad user requests can. If we do not ignore our health checks they may artificially raise our Apdex score hiding real problems in a fog of fake successes. We will ignore requests to our health check handler.</span></p><p class="c9"><span class="c0 c3">Translating this to PromQL we get</span></p><p class="c2"><span class="c1">- record: job:http_request_apdex:max</span></p><p class="c2"><span class="c1">&nbsp;expr: |</span></p><p class="c2"><span class="c1">&nbsp; &nbsp;(</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp;sum(</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp;rate( &nbsp; &nbsp; &nbsp; &nbsp;</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;http_request_duration_seconds_bucket{</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;handler!=&quot;/healthz&quot;,</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;code!~&quot;5..&quot;}[1m]</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp;)</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp;* ON(job, le) GROUP_LEFT()</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp;job_le:http_request_apdex_target_seconds:max</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp;) BY (job)</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp;+</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp;sum(</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp;rate(</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;http_request_duration_seconds_bucket{</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;handler!=&quot;/healthz&quot;,</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;code!~&quot;5..&quot;}[1m]</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp;)</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp;* ON(job, le) GROUP_LEFT()</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp;job_le:http_request_apdex_target_seconds:4_times_max</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp;) BY (job)</span></p><p class="c2"><span class="c1">&nbsp; &nbsp;) / 2</span></p><p class="c2"><span class="c1">&nbsp; &nbsp;/</span></p><p class="c2"><span class="c1">&nbsp; &nbsp;sum(</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp;rate(</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp;http_request_duration_seconds_count{</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;handler!=&quot;/healthz&quot;}[1m]</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp;)</span></p><p class="c2"><span class="c1">&nbsp; &nbsp;) BY (job)</span></p><p class="c9"><span class="c0">This query can be divided up into simpler sections. First we calculate the rate of satisfactory requests. As per the rules given in the first section, this is made up of all successful (non-5xx) requests that have been sampled by our </span><span class="c4">le=&rdquo;T&rdquo; </span><span class="c0">bucket. We then calculate our tolerable requests from the </span><span class="c4">4T </span><span class="c0">bucket. Finally we divide the sum of those by 2, and then by the total number of requests to the service. The bucket selection is achieved through label matching on our </span><span class="c4">le </span><span class="c0">label</span><span class="c4">.</span><span class="c0 c3">The GROUP_LEFT clauses are used to retain any additional instrumentation labels that were present on the original time series.</span></p><p class="c9"><span class="c0 c3">Finally we can create an alert.</span></p><p class="c2"><span class="c1">- alert: HTTPApdexViolation</span></p><p class="c2"><span class="c1">&nbsp;expr: job:http_request_apdex:max &lt; 0.8</span></p><p class="c2"><span class="c1">&nbsp;for: 5m</span></p><p class="c2"><span class="c1">&nbsp;annotations:</span></p><p class="c2"><span class="c1">&nbsp; &nbsp;description: |</span></p><p class="c2"><span class="c1">&nbsp; &nbsp; &nbsp;&#39;{{ $labels.job }} has ApDex has dropped below 0.8 {{printf &quot;%.2f&quot; $value}}&#39;</span></p><p class="c9"><span class="c0">We could allow the alert threshold, here set to 0.8, to be specified by the application by exposing it as a metrics, as we did for the </span><span class="c4">T</span><span class="c0 c3">&nbsp;value. To do so though would encourage developers to lower the alert threshold in response to an application which is not meeting its Apdex target. In my opinion it is more honest to adjust the target response time rather than the alert threshold. This encourages discussions around what is really achievable, rather than simply twiddling a &ldquo;magic alert threshold&rdquo;.</span></p><h2 class="c15" id="h.oxd20614cnup"><span class="c3 c17">Conclusion</span></h2><p class="c10"><span class="c0 c3">Apdex is a useful metric for monitoring, and especially for alerting. It is a concise representation of user experience. Prometheus provides us all the tools we need to calculate Apdex. The same technique presented here can be used to estimate similarly &ldquo;Business-centric&rdquo; metrics, such as error-budgets and service availability.</span></p><p class="c18"><span class="c14 c3"></span></p></body></html>